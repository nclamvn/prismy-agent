# === AI Translator Agent: app.py - PHI√äN B·∫¢N CHUY√äN NGHI·ªÜP ===
import streamlit as st
import fitz
import os
import asyncio
import httpx
import tiktoken
import time
import hashlib
import pickle
from io import BytesIO
from docx import Document
from docx.shared import Pt
from reportlab.pdfgen import canvas
from reportlab.lib.pagesizes import A4
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont
from reportlab.lib.utils import simpleSplit
from difflib import SequenceMatcher
import nest_asyncio
from langdetect import detect
import pandas as pd
import re
from datetime import datetime
from logger_utils import log_model_error
from model_router_async import call_model_async
from api_keys import API_KEYS

nest_asyncio.apply()

# === Font PDF ===
FONT_PATH = "NotoSans-Regular.ttf"
FONT_NAME = "NotoSans"
if os.path.exists(FONT_PATH):
    pdfmetrics.registerFont(TTFont(FONT_NAME, FONT_PATH))
else:
    st.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y font NotoSans-Regular.ttf.")

# === DANH S√ÅCH NG√îN NG·ªÆ PH·ªî BI·∫æN ===
SUPPORTED_LANGUAGES = {
    "Vietnamese": "üáªüá≥ Ti·∫øng Vi·ªát",
    "English": "üá∫üá∏ English", 
    "Chinese": "üá®üá≥ ‰∏≠Êñá",
    "Japanese": "üáØüáµ Êó•Êú¨Ë™û",
    "Korean": "üá∞üá∑ ÌïúÍµ≠Ïñ¥",
    "French": "üá´üá∑ Fran√ßais",
    "German": "üá©üá™ Deutsch",
    "Spanish": "üá™üá∏ Espa√±ol",
    "Thai": "üáπüá≠ ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢",
    "Arabic": "üá∏üá¶ ÿßŸÑÿπÿ±ÿ®Ÿäÿ©"
}

# === H√†m x·ª≠ l√Ω vƒÉn b·∫£n v√† d·ªãch ===
def remove_overlap(prev, curr):
    matcher = SequenceMatcher(None, prev[-200:], curr)
    match = matcher.find_longest_match(0, len(prev[-200:]), 0, len(curr))
    return curr[match.b + match.size:] if match.size > 30 else curr

def count_tokens(text, model="gpt-3.5-turbo"):
    enc = tiktoken.get_encoding("cl100k_base")
    return len(enc.encode(text))

def estimate_cost(text, model):
    tokens = count_tokens(text, model)
    rate = {
        "gpt-3.5-turbo": 0.0035,
        "gpt-4.1-mini": 0.005,
        "gpt-4.1": 0.03,
        "gpt-4": 0.06,
        "gpt-4o": 0.01,
        "gpt-4.5": 0.06
    }
    for k in rate:
        if k in model:
            return tokens, round(tokens / 1000 * rate[k], 4)
    return tokens, round(tokens / 1000 * 0.03, 4)

def split_text(text, max_length=2000, overlap=200):
    words = text.split()
    chunks, i = [], 0
    while i < len(words):
        chunks.append(" ".join(words[i:i + max_length]))
        i += max_length - overlap
    return chunks

def build_prompt(para: str, target_lang: str, style_level: str) -> str:
    prompt = f"Translate this text to {target_lang}:\n\n{para}"
    lvl = style_level.lower()

    if "chu·∫©n" in lvl and "in ·∫•n" not in lvl:
        prompt += "\n\nStyle: smooth, fluent, professional."
    elif "cao c·∫•p" in lvl or "chu·∫©n in ·∫•n" in lvl:
        prompt += "\n\nStyle: publication-quality, accurate, natural."
    else:
        prompt += "\n\nStyle: rough, simple draft."

    return prompt

async def translate_paragraphs(
    paragraphs: list[str],
    style_level: str,
    target_lang: str,
    model_name: str
) -> tuple[list[str], list[int]]:
    results: list[str | None] = [None] * len(paragraphs)
    errors: list[int] = []

    async def translate_one(idx: int, para: str) -> None:
        try:
            prompt = build_prompt(para, target_lang, style_level)
            messages = [{"role": "user", "content": prompt}]

            result = await call_model_async(style_level, messages)

            if detect(result[:200]) == detect(para[:200]):
                raise ValueError("Model did not translate.")

            results[idx] = result
        except Exception as exc:
            results[idx] = f"[L·ªói ƒëo·∫°n {idx + 1}]"
            errors.append(idx)
            log_model_error(
                {"model": style_level, "provider": "mixed"},
                f"[APP] Exception ƒëo·∫°n {idx + 1}: {exc}",
                tag="APP"
            )

    await asyncio.gather(*(translate_one(i, p) for i, p in enumerate(paragraphs)))

    results = [
        r if r is not None else f"[L·ªói ƒëo·∫°n {i + 1}]"
        for i, r in enumerate(results)
    ]
    return results, errors

def export_word(paragraphs):
    doc = Document()
    doc.add_heading("B·∫£n d·ªãch t√†i li·ªáu", level=1)
    for para in paragraphs:
        if '[L·ªói ƒëo·∫°n' in para:
            continue
        p = doc.add_paragraph()
        run = p.add_run(para)
        run.font.size = Pt(12)
        p.paragraph_format.space_after = Pt(6)
    buf = BytesIO()
    doc.save(buf)
    buf.seek(0)
    return buf

def export_pdf(paragraphs):
    buf = BytesIO()
    c = canvas.Canvas(buf, pagesize=A4)
    width, height = A4
    margin = 40
    max_width = width - 2 * margin
    y = height - margin
    line_height = 16

    try:
        c.setFont(FONT_NAME, 12)
    except:
        c.setFont("Helvetica", 12)

    for para in paragraphs:
        if '[L·ªói ƒëo·∫°n' in para:
            continue
        lines = simpleSplit(para, FONT_NAME, 12, max_width)
        for line in lines:
            if y < margin + line_height:
                c.showPage()
                y = height - margin
                c.setFont(FONT_NAME, 12)
            c.drawString(margin, y, line)
            y -= line_height
        y -= line_height

    c.save()
    buf.seek(0)
    return buf

def get_file_hash(file_bytes):
    return hashlib.sha256(file_bytes).hexdigest()

def save_translation_cache(
        file_hash: str,
        model: str,
        target_lang: str,
        result: list[str],
        word_buf: BytesIO,
        pdf_buf: BytesIO
) -> None:
    os.makedirs("cache", exist_ok=True)
    cache_key = f"{file_hash}_{model}_{target_lang}"
    with open(f"cache/{cache_key}.pkl", "wb") as f:
        pickle.dump((result, word_buf.getvalue(), pdf_buf.getvalue()), f)

def load_translation_cache(
        file_hash: str,
        model: str,
        target_lang: str
) -> tuple[list[str] | None, BytesIO | None, BytesIO | None]:
    cache_key = f"{file_hash}_{model}_{target_lang}"
    try:
        with open(f"cache/{cache_key}.pkl", "rb") as f:
            result, word_bytes, pdf_bytes = pickle.load(f)
            return result, BytesIO(word_bytes), BytesIO(pdf_bytes)
    except FileNotFoundError:
        return None, None, None

def parse_failed_log():
    rows = []
    try:
        with open("logs/failed_model.log", "r", encoding="utf-8") as f:
            for line in f:
                match = re.match(r"\[(.*?)\] ‚ùå \[(.*?)\] (\w+) - (.*?): (.*)", line)
                if match:
                    time, tag, provider, model, msg = match.groups()
                    rows.append({
                        "timestamp": time,
                        "tag": tag,
                        "provider": provider,
                        "model": model,
                        "message": msg
                    })
    except FileNotFoundError:
        pass
    return pd.DataFrame(rows)

def parse_benchmark_log():
    rows = []
    try:
        with open("logs/benchmark.log", "r", encoding="utf-8") as f:
            for line in f:
                match = re.match(r"\[(.*?)\] ‚úÖ BENCHMARK (\w+) - (.*?): ([\d.]+)s.*?(\d+) tokens", line)
                if match:
                    time, provider, model, duration, tokens = match.groups()
                    rows.append({
                        "timestamp": time,
                        "provider": provider,
                        "model": model,
                        "duration": float(duration),
                        "tokens": int(tokens)
                    })
    except FileNotFoundError:
        pass
    return pd.DataFrame(rows)

# === GIAO DI·ªÜN CH√çNH - PHI√äN B·∫¢N CHUY√äN NGHI·ªÜP ===
st.set_page_config(
    page_title="AI Translator Agent", 
    layout="wide",
    initial_sidebar_state="collapsed"
)

# === HEADER CHUY√äN NGHI·ªÜP ===
st.markdown("""
<div style='text-align: center; padding: 1rem 0; background: linear-gradient(90deg, #667eea 0%, #764ba2 100%); border-radius: 10px; margin-bottom: 2rem;'>
    <h1 style='color: white; margin: 0; font-size: 2.5rem; font-weight: bold;'>
        üåê AI Translator Agent
    </h1>
    <p style='color: #f0f2f6; margin: 0.5rem 0 0 0; font-size: 1.2rem;'>
        D·ªãch t√†i li·ªáu th√¥ng minh v·ªõi c√¥ng ngh·ªá AI ti√™n ti·∫øn
    </p>
</div>
""", unsafe_allow_html=True)

# === TABS CH√çNH ===
tab1, tab2 = st.tabs(["üìò D·ªãch t√†i li·ªáu", "üìä Theo d√µi h·ªá th·ªëng"])

# === TAB 1: D·ªäCH T√ÄI LI·ªÜU ===
with tab1:
    # === 1. NH·∫¨P API KEY (ƒê·ª®NG ƒê·∫¶U) ===
    st.markdown("### üîë C·∫•u h√¨nh API")
    api_key = st.text_input(
        "Nh·∫≠p OpenAI API Key c·ªßa b·∫°n:", 
        type="password",
        placeholder="sk-...",
        help="API key s·∫Ω ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ g·ªçi c√°c m√¥ h√¨nh GPT"
    )
    if api_key and api_key.strip() and api_key.strip() not in API_KEYS["openai"]:
        API_KEYS["openai"].insert(0, api_key.strip())
        st.session_state["user_key_added"] = True

    st.divider()

    # === 2. CH·ªåN CH·∫æ ƒê·ªò D·ªäCH & M√î H√åNH ===
    st.markdown("### üéØ C·∫•u h√¨nh d·ªãch thu·∫≠t")
    
    col1, col2 = st.columns(2)
    with col1:
        style = st.selectbox(
            "Ch·∫ø ƒë·ªô d·ªãch:",
            ["Th√¥", "Chu·∫©n", "Chu·∫©n in ·∫•n"],
            help="‚Ä¢ Th√¥: Nhanh, chi ph√≠ th·∫•p\n‚Ä¢ Chu·∫©n: C√¢n b·∫±ng ch·∫•t l∆∞·ª£ng-chi ph√≠\n‚Ä¢ Chu·∫©n in ·∫•n: Ch·∫•t l∆∞·ª£ng cao nh·∫•t"
        )
    
    with col2:
        default_model = {
            "Th√¥": "gpt-3.5-turbo",
            "Chu·∫©n": "gpt-4o", 
            "Chu·∫©n in ·∫•n": "gpt-4.5"
        }[style]
        
        model = st.selectbox(
            "M√¥ h√¨nh AI:",
            ["gpt-3.5-turbo", "gpt-4.1-mini", "gpt-4.1", "gpt-4", "gpt-4o", "gpt-4.5"],
            index=["gpt-3.5-turbo", "gpt-4.1-mini", "gpt-4.1", "gpt-4", "gpt-4o", "gpt-4.5"].index(default_model)
        )

    st.divider()

    # === 3. T·∫¢I FILE ===
    st.markdown("### üìÑ T·∫£i t√†i li·ªáu")
    uploaded_files = st.file_uploader(
        "Ch·ªçn file PDF c·∫ßn d·ªãch:",
        type=["pdf"], 
        accept_multiple_files=True,
        help="H·ªó tr·ª£ nhi·ªÅu file PDF, t·ªëi ƒëa 200MB m·ªói file"
    )

    if uploaded_files:
        for uploaded_file in uploaded_files:
            st.markdown(f"""
            <div style='background: #f8f9fa; padding: 1rem; border-radius: 8px; margin: 1rem 0; border-left: 4px solid #007bff;'>
                <h4 style='margin: 0; color: #007bff;'>üìÑ {uploaded_file.name}</h4>
            </div>
            """, unsafe_allow_html=True)
            
            file_bytes = uploaded_file.read()
            file_hash = get_file_hash(file_bytes)

            try:
                doc = fitz.open(stream=file_bytes, filetype="pdf")
                pages_text = [page.get_text("text") for page in doc]
                full_text = "\n\n".join(pages_text)
                doc.close()
            except Exception as e:
                st.error(f"‚ùå Kh√¥ng th·ªÉ x·ª≠ l√Ω file {uploaded_file.name}: {e}")
                continue

            # === 4. PH√ÅT HI·ªÜN NG√îN NG·ªÆ ===
            st.markdown("#### üåê Ph√°t hi·ªán ng√¥n ng·ªØ")
            try:
                detected_lang = detect(full_text[:500])
                lang_name = {
                    'en': 'English üá∫üá∏', 'vi': 'Vietnamese üáªüá≥', 'zh': 'Chinese üá®üá≥',
                    'ja': 'Japanese üáØüáµ', 'ko': 'Korean üá∞üá∑', 'fr': 'French üá´üá∑',
                    'de': 'German üá©üá™', 'es': 'Spanish üá™üá∏', 'th': 'Thai üáπüá≠', 'ar': 'Arabic üá∏üá¶'
                }.get(detected_lang, f'{detected_lang.upper()} üåç')
                
                st.success(f"**Ng√¥n ng·ªØ ph√°t hi·ªán:** {lang_name}")
            except:
                st.warning("‚ö†Ô∏è Kh√¥ng th·ªÉ ph√°t hi·ªán ng√¥n ng·ªØ t·ª± ƒë·ªông")

            # === 5. CH·ªåN NG√îN NG·ªÆ ƒê√çCH ===
            st.markdown("#### üéØ Ch·ªçn ng√¥n ng·ªØ ƒë√≠ch")
            target_lang = st.selectbox(
                "D·ªãch sang:",
                list(SUPPORTED_LANGUAGES.keys()),
                format_func=lambda x: SUPPORTED_LANGUAGES[x],
                key=f"target_{uploaded_file.name}"
            )

            # === 6. ∆Ø·ªöC L∆Ø·ª¢NG CHI PH√ç ===
            tokens, cost = estimate_cost(full_text, model)
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("üìä Tokens", f"{tokens:,}")
            with col2:
                st.metric("üí∞ Chi ph√≠ ∆∞·ªõc t√≠nh", f"${cost:.4f}")
            with col3:
                st.metric("üìÑ S·ªë trang", f"{len(pages_text)}")

            # === 7. KI·ªÇM TRA CACHE ===
            cached_result, cached_word, cached_pdf = load_translation_cache(
                file_hash, model, target_lang
            )

            if cached_result:
                st.success("‚úÖ ƒê√£ c√≥ b·∫£n d·ªãch trong cache - ti·∫øt ki·ªám chi ph√≠!")
                
                # Hi·ªÉn th·ªã m·∫´u t·ª´ cache
                st.markdown("#### üëÄ Xem tr∆∞·ªõc b·∫£n d·ªãch")
                preview_chunks = cached_result[:3] if len(cached_result) >= 3 else cached_result
                
                for i, chunk in enumerate(preview_chunks):
                    if '[L·ªói ƒëo·∫°n' in chunk:
                        continue
                    st.markdown(f"**üìù ƒêo·∫°n {i+1}:**")
                    st.markdown(f"""
                    <div style='height: 200px; overflow-y: auto; padding: 1rem; background: #f8f9fa; 
                         border: 1px solid #dee2e6; border-radius: 6px; font-family: "Times New Roman", serif;'>
                        {chunk.replace(chr(10), '<br>')}
                    </div>
                    """, unsafe_allow_html=True)
                    st.markdown("---")

                # N√∫t t·∫£i v·ªÅ + Google Drive
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.download_button(
                        "üì• T·∫£i Word",
                        cached_word,
                        file_name=f"{uploaded_file.name}_translated.docx",
                        mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
                    )
                with col2:
                    st.download_button(
                        "üì• T·∫£i PDF", 
                        cached_pdf,
                        file_name=f"{uploaded_file.name}_translated.pdf",
                        mime="application/pdf"
                    )
                with col3:
                    if st.button("‚òÅÔ∏è L∆∞u Google Drive", key=f"gdrive_{uploaded_file.name}"):
                        st.info("üîÑ T√≠nh nƒÉng Google Drive s·∫Ω s·ªõm ra m·∫Øt!")
                with col4:
                    if st.button("üîÑ D·ªãch l·∫°i", key=f"retranslate_{uploaded_file.name}"):
                        st.rerun()

                st.info("üí° **Mu·ªën d·ªãch l·∫°i?** Thay ƒë·ªïi ch·∫ø ƒë·ªô d·ªãch ho·∫∑c m√¥ h√¨nh AI ·ªü tr√™n r·ªìi nh·∫•n 'D·ªãch l·∫°i'")

            else:
                # === 8. N√öT D·ªäCH ===
                if st.button(
                    f"üöÄ B·∫Øt ƒë·∫ßu d·ªãch {uploaded_file.name}",
                    type="primary",
                    use_container_width=True
                ):
                    # Thanh ti·∫øn tr√¨nh
                    progress_bar = st.progress(0)
                    status_text = st.empty()
                    
                    with st.spinner("üîÑ ƒêang x·ª≠ l√Ω t√†i li·ªáu..."):
                        start_time = time.time()
                        
                        # C·∫≠p nh·∫≠t ti·∫øn tr√¨nh
                        progress_bar.progress(20)
                        status_text.text("üìù ƒêang chia nh·ªè vƒÉn b·∫£n...")
                        
                        paragraphs = split_text(full_text)
                        
                        progress_bar.progress(40)
                        status_text.text("ü§ñ ƒêang d·ªãch v·ªõi AI...")
                        
                        # FIX: D√πng asyncio.run
                        translated, errors = asyncio.get_event_loop().run_until_complete(
                            translate_paragraphs(paragraphs, style, target_lang, model)
                        )

                        if not translated or not isinstance(translated, list) or not translated[0]:
                            st.error("‚ùå Kh√¥ng c√≥ k·∫øt qu·∫£ d·ªãch h·ª£p l·ªá.")
                            continue

                        progress_bar.progress(70)
                        status_text.text("üîß ƒêang t·ªëi ∆∞u k·∫øt qu·∫£...")
                        
                        # X·ª≠ l√Ω overlap
                        final_result = [translated[0]]
                        for i in range(1, len(translated)):
                            final_result.append(remove_overlap(final_result[-1], translated[i]))

                        progress_bar.progress(90)
                        status_text.text("üìÑ ƒêang t·∫°o file...")
                        
                        word_buf = export_word(final_result)
                        pdf_buf = export_pdf(final_result)
                        save_translation_cache(
                            file_hash, model, target_lang, final_result, word_buf, pdf_buf
                        )

                        progress_bar.progress(100)
                        duration = round(time.time() - start_time, 1)
                        status_text.text(f"‚úÖ Ho√†n th√†nh trong {duration} gi√¢y!")

                    # === 9. HI·ªÇN TH·ªä K·∫æT QU·∫¢ ===
                    st.success(f"üéâ **D·ªãch ho√†n t·∫•t!** Th·ªùi gian: **{duration} gi√¢y**")
                    
                    if errors:
                        st.warning(f"‚ö†Ô∏è C√≥ {len(errors)} ƒëo·∫°n b·ªã l·ªói trong qu√° tr√¨nh d·ªãch")

                    # === 10. C·ª¨A S·ªî XEM TH·ª¨ ===
                    st.markdown("#### üëÄ Xem tr∆∞·ªõc k·∫øt qu·∫£ d·ªãch")
                    
                    # Ch·ªçn s·ªë ƒëo·∫°n hi·ªÉn th·ªã th√¥ng minh
                    total_chunks = len(final_result)
                    if total_chunks <= 1:
                        preview_chunks = final_result
                        st.info("üìÑ T√†i li·ªáu ng·∫Øn - hi·ªÉn th·ªã to√†n b·ªô")
                    elif total_chunks <= 3:
                        preview_chunks = final_result
                        st.info(f"üìÑ Hi·ªÉn th·ªã {total_chunks} ƒëo·∫°n")
                    else:
                        preview_chunks = final_result[:3]
                        st.info(f"üìÑ Hi·ªÉn th·ªã 3/{total_chunks} ƒëo·∫°n ƒë·∫ßu ti√™n")

                    for i, para in enumerate(preview_chunks):
                        if '[L·ªói ƒëo·∫°n' in para:
                            continue
                        st.markdown(f"**üìù ƒêo·∫°n {i+1}:**")
                        st.markdown(f"""
                        <div style='height: 300px; overflow-y: auto; padding: 1rem; background: #f8f9fa; 
                             border: 1px solid #dee2e6; border-radius: 6px; font-family: "Times New Roman", serif;
                             line-height: 1.6;'>
                            {para.replace(chr(10), '<br>')}
                        </div>
                        """, unsafe_allow_html=True)
                        st.markdown("---")

                    # === 11. N√öT T·∫¢I V·ªÄ & GOOGLE DRIVE ===
                    st.markdown("#### üíæ L∆∞u tr·ªØ & T·∫£i v·ªÅ")
                    
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.download_button(
                            "üì• T·∫£i file Word",
                            word_buf,
                            file_name=f"{uploaded_file.name}_translated.docx",
                            mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                            use_container_width=True
                        )
                    with col2:
                        st.download_button(
                            "üì• T·∫£i file PDF",
                            pdf_buf,
                            file_name=f"{uploaded_file.name}_translated.pdf",
                            mime="application/pdf",
                            use_container_width=True
                        )
                    with col3:
                        if st.button("‚òÅÔ∏è L∆∞u Google Drive", use_container_width=True):
                            st.info("üîÑ T√≠nh nƒÉng Google Drive ƒëang ph√°t tri·ªÉn!")

                    # === 12. TH√îNG B√ÅO D·ªäCH L·∫†I ===
                    st.markdown("""
                    <div style='background: #e3f2fd; padding: 1rem; border-radius: 8px; border-left: 4px solid #2196f3; margin-top: 1rem;'>
                        <strong>üí° Kh√¥ng h√†i l√≤ng v·ªõi k·∫øt qu·∫£?</strong><br>
                        B·∫°n c√≥ th·ªÉ thay ƒë·ªïi <strong>ch·∫ø ƒë·ªô d·ªãch</strong> ho·∫∑c <strong>m√¥ h√¨nh AI</strong> ·ªü ph√≠a tr√™n, 
                        sau ƒë√≥ nh·∫•n <strong>"D·ªãch l·∫°i"</strong> ƒë·ªÉ c√≥ k·∫øt qu·∫£ t·ªët h∆°n.
                    </div>
                    """, unsafe_allow_html=True)

# === TAB 2: DASHBOARD (GI·ªÆ NGUY√äN) ===
with tab2:
    st.subheader("üìà Hi·ªáu su·∫•t m√¥ h√¨nh")
    df_bench = parse_benchmark_log()
    if df_bench.empty:
        st.warning("Ch∆∞a c√≥ d·ªØ li·ªáu benchmark.")
    else:
        avg = df_bench.groupby(["provider", "model"]).agg({"duration": "mean", "tokens": "mean"}).round(2)
        st.dataframe(avg, use_container_width=True)

        count_df = df_bench.groupby(["provider", "model"]).size().reset_index(name="calls")
        st.bar_chart(count_df.set_index("model")["calls"])

        st.caption("üßæ C√°c b·∫£n ghi benchmark g·∫ßn nh·∫•t")
        st.dataframe(df_bench.sort_values("timestamp", ascending=False).head(10), use_container_width=True)

    st.subheader("‚ö†Ô∏è Th·ªëng k√™ l·ªói API")
    df_fail = parse_failed_log()
    if df_fail.empty:
        st.success("Kh√¥ng c√≥ l·ªói n√†o ƒë∆∞·ª£c ghi nh·∫≠n.")
    else:
        error_stats = df_fail.groupby(["provider", "model", "tag"]).size().reset_index(name="errors")
        st.dataframe(error_stats, use_container_width=True)

        st.caption("‚ùå C√°c l·ªói g·∫ßn nh·∫•t")
        st.dataframe(df_fail.sort_values("timestamp", ascending=False).head(10), use_container_width=True)