"""
PRISM Enhanced Language Detection
Ultra-sophisticated multi-modal language identification
"""

import re
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass

@dataclass
class LanguageSignature:
    """Comprehensive language signature"""
    language: str
    confidence: float
    evidence: List[str]
    character_patterns: Dict[str, float]
    linguistic_patterns: Dict[str, float]
    cultural_patterns: Dict[str, float]

class EnhancedLanguageDetection:
    """
    Ultra-sophisticated language detection using multiple modalities
    """
    
    def __init__(self):
        self.language_signatures = self._build_language_signatures()
        self.cultural_indicators = self._build_cultural_indicators()
        self.linguistic_patterns = self._build_linguistic_patterns()
        
    def _build_language_signatures(self) -> Dict[str, Dict]:
        """Build comprehensive language signatures"""
        return {
            "vietnamese": {
                "special_chars": "√†√°·∫°·∫£√£√¢·∫ß·∫•·∫≠·∫©·∫´ƒÉ·∫±·∫Ø·∫∑·∫≥·∫µ√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªá·ªÉ·ªÖ√¨√≠·ªã·ªâƒ©√≤√≥·ªç·ªè√µ√¥·ªì·ªë·ªô·ªï·ªó∆°·ªù·ªõ·ª£·ªü·ª°√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª±·ª≠·ªØ·ª≥√Ω·ªµ·ª∑·ªπƒë",
                "tone_markers": "√†√°·∫°·∫£√£√¢·∫ß·∫•·∫≠·∫©·∫´ƒÉ·∫±·∫Ø·∫∑·∫≥·∫µ√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªá·ªÉ·ªÖ√¨√≠·ªã·ªâƒ©√≤√≥·ªç·ªè√µ√¥·ªì·ªë·ªô·ªï·ªó∆°·ªù·ªõ·ª£·ªü·ª°√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª±·ª≠·ªØ·ª≥√Ω·ªµ·ª∑·ªπ",
                "unique_letters": "ƒëƒÉ√¢√™√¥∆°∆∞",
                "character_ranges": [(0x00C0, 0x00FF), (0x1EA0, 0x1EF9)],
                "grammatical_markers": [
                    r'\b(?:l√†|c√≥|ƒë∆∞·ª£c|ƒë√£|s·∫Ω|ƒëang|r·ªìi|m√†|th√¨|n√™n)\b',
                    r'\b(?:m·ªôt|hai|ba|b·ªën|nƒÉm|s√°u|b·∫£y|t√°m|ch√≠n|m∆∞·ªùi)\b',
                    r'\b(?:n√†y|ƒë√≥|kia|n√†o|g√¨|ai|ƒë√¢u|khi|n√†o)\b'
                ]
            },
            
            "english": {
                "special_chars": "",
                "tone_markers": "",
                "unique_letters": "",
                "character_ranges": [(0x0041, 0x005A), (0x0061, 0x007A)],
                "grammatical_markers": [
                    r'\b(?:is|are|was|were|have|has|had|will|would|can|could|should)\b',
                    r'\b(?:the|a|an|and|or|but|if|when|where|how|why|what)\b',
                    r'\b(?:one|two|three|four|five|six|seven|eight|nine|ten)\b'
                ]
            },
            
            "chinese": {
                "special_chars": "ÁöÑÊòØ‰∫ÜÊàë‰∏ç‰∫∫Âú®‰ªñÊúâÈÄôÂÄã‰∏äÂÄë‰æÜÂà∞ÊôÇÂ§ßÂú∞ÁÇ∫Â≠ê‰∏≠‰Ω†Ë™™ÁîüÂúãÂπ¥ËëóÂ∞±ÈÇ£ÂíåË¶ÅÂ•πÂá∫‰πüÂæóË£°ÂæåËá™‰ª•ÊúÉÂÆ∂ÂèØ‰∏ãËÄåÈÅéÂ§©ÂéªËÉΩÂ∞çÂ∞èÂ§öÁÑ∂ÊñºÂøÉÂ≠∏È∫º‰πãÈÉΩÂ•ΩÁúãËµ∑ÁôºÁï∂Ê≤íÊàêÂè™Â¶Ç‰∫ãÊääÈÇÑÁî®Á¨¨Ê®£ÈÅìÊÉ≥‰ΩúÁ®ÆÈñãË¶ÅÂõ†ÁÇ∫Â∞±ÊòØÈÄôÈ∫ºÊ®£Â≠ê",
                "tone_markers": "",
                "unique_letters": "",
                "character_ranges": [(0x4E00, 0x9FFF), (0x3400, 0x4DBF)],
                "grammatical_markers": [r'[ÁöÑ‰∫ÜÊòØÂú®ÊúâÈÄôÂÄãÈÇ£ÂíåË¶Å‰πüÂ∞±ÈÉΩËÉΩÂèØÊúÉË¶ÅÂõ†ÁÇ∫‰ªÄÈ∫ºÊÄéÈ∫ºÊ®£]']
            },
            
            "korean": {
                "special_chars": "Í∞ÄÎÇòÎã§ÎùºÎßàÎ∞îÏÇ¨ÏïÑÏûêÏ∞®Ïπ¥ÌÉÄÌååÌïò",
                "tone_markers": "",
                "unique_letters": "",
                "character_ranges": [(0xAC00, 0xD7AF), (0x1100, 0x11FF)],
                "grammatical_markers": [r'[Ïù¥Í∞ÄÏùÄÎäîÏùÑÎ•ºÍ≥ºÏôÄÏùòÏóêÏÑúÎ°úÎ∂ÄÌÑ∞ÍπåÏßÄ]']
            },
            
            "japanese": {
                "special_chars": "„ÅÇ„ÅÑ„ÅÜ„Åà„Åä„Åã„Åç„Åè„Åë„Åì„Åï„Åó„Åô„Åõ„Åù„Åü„Å°„Å§„Å¶„Å®„Å™„Å´„Å¨„Å≠„ÅÆ„ÅØ„Å≤„Åµ„Å∏„Åª„Åæ„Åø„ÇÄ„ÇÅ„ÇÇ„ÇÑ„ÇÜ„Çà„Çâ„Çä„Çã„Çå„Çç„Çè„Çí„Çì",
                "tone_markers": "",
                "unique_letters": "",
                "character_ranges": [(0x3040, 0x309F), (0x30A0, 0x30FF), (0x4E00, 0x9FFF)],
                "grammatical_markers": [r'[„ÅØ„Åå„Çí„Å´„Åß„Å®„Åã„Çâ„Åæ„Åß]', r'„Åß„Åô|„Åæ„Åô|„Å†|„Åß„ÅÇ„Çã']
            }
        }
    
    def _build_cultural_indicators(self) -> Dict[str, List[str]]:
        """Build cultural context indicators"""
        return {
            "vietnamese": [
                # Names and titles
                r'\b(?:anh|em|ch·ªã|√¥ng|b√†|c√¥|ch√∫|th·∫ßy)\s+[A-Z√Ä√Å·∫†·∫¢√É√Ç·∫¶·∫§·∫¨·∫®·∫™ƒÇ·∫∞·∫Æ·∫∂·∫≤·∫¥√à√â·∫∏·∫∫·∫º√ä·ªÄ·∫æ·ªÜ·ªÇ·ªÑ√å√ç·ªä·ªàƒ®√í√ì·ªå·ªé√ï√î·ªí·ªê·ªò·ªî·ªñ∆†·ªú·ªö·ª¢·ªû·ª†√ô√ö·ª§·ª¶≈®∆Ø·ª™·ª®·ª∞·ª¨·ªÆ·ª≤√ù·ª¥·ª∂·ª∏ƒê][a-z√†√°·∫°·∫£√£√¢·∫ß·∫•·∫≠·∫©·∫´ƒÉ·∫±·∫Ø·∫∑·∫≥·∫µ√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªá·ªÉ·ªÖ√¨√≠·ªã·ªâƒ©√≤√≥·ªç·ªè√µ√¥·ªì·ªë·ªô·ªï·ªó∆°·ªù·ªõ·ª£·ªü·ª°√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª±·ª≠·ªØ·ª≥√Ω·ªµ·ª∑·ªπƒë]+',
                # Common Vietnamese names
                r'\b(?:Minh|Lan|H∆∞∆°ng|Linh|Anh|Dung|H·∫£i|Nam|Mai|Hoa|Tu·∫•n|H√πng|Long|Phong|ƒê·ª©c|Khang|Th√†nh|Ho√†ng|Quang|Thu|Loan|Nga|Th·∫£o|Trang|Vy)\b',
                # Places
                r'\b(?:Vi·ªát\s+Nam|H√†\s+N·ªôi|Tp\.\s+H·ªì\s+Ch√≠\s+Minh|S√†i\s+G√≤n|ƒê√†\s+N·∫µng|Hu·∫ø|C·∫ßn\s+Th∆°|H·∫£i\s+Ph√≤ng|Nha\s+Trang|ƒê√†\s+L·∫°t)\b',
                # Cultural terms
                r'\b(?:√°o\s+d√†i|b√°nh\s+m√¨|ph·ªü|b√∫n\s+b√≤|c∆°m\s+t·∫•m|ch·∫£\s+c√°|g·ªèi\s+cu·ªën|nem\s+r√°n|b√∫n\s+ch·∫£|b√°nh\s+x√®o)\b',
                # Time expressions
                r'\b(?:h√¥m\s+nay|ng√†y\s+mai|h√¥m\s+qua|tu·∫ßn\s+sau|th√°ng\s+tr∆∞·ªõc|nƒÉm\s+nay|gi·ªù\s+n√†y|l√∫c\s+n√†o)\b'
            ],
            
            "english": [
                # Common names
                r'\b(?:John|Mary|James|Sarah|Michael|Lisa|David|Karen|Robert|Susan|William|Jessica|Richard|Nancy|Joseph|Ashley)\b',
                # Places
                r'\b(?:New\s+York|Los\s+Angeles|Chicago|Houston|Phoenix|Philadelphia|San\s+Antonio|San\s+Diego|Dallas|San\s+Jose|Austin|Jacksonville)\b',
                # Cultural terms
                r'\b(?:hamburger|pizza|hot\s+dog|sandwich|coffee|tea|breakfast|lunch|dinner|weekend|holiday)\b',
                # Time expressions
                r'\b(?:today|tomorrow|yesterday|next\s+week|last\s+month|this\s+year|right\s+now|sometime)\b'
            ]
        }
    
    def _build_linguistic_patterns(self) -> Dict[str, List[str]]:
        """Build linguistic structure patterns"""
        return {
            "vietnamese": [
                # Word order patterns (SVO)
                r'\b[A-Z√Ä√Å·∫†·∫¢√É√Ç·∫¶·∫§·∫¨·∫®·∫™ƒÇ·∫∞·∫Æ·∫∂·∫≤·∫¥√à√â·∫∏·∫∫·∫º√ä·ªÄ·∫æ·ªÜ·ªÇ·ªÑ√å√ç·ªä·ªàƒ®√í√ì·ªå·ªé√ï√î·ªí·ªê·ªò·ªî·ªñ∆†·ªú·ªö·ª¢·ªû·ª†√ô√ö·ª§·ª¶≈®∆Ø·ª™·ª®·ª∞·ª¨·ªÆ·ª≤√ù·ª¥·ª∂·ª∏ƒê][a-z√†√°·∫°·∫£√£√¢·∫ß·∫•·∫≠·∫©·∫´ƒÉ·∫±·∫Ø·∫∑·∫≥·∫µ√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªá·ªÉ·ªÖ√¨√≠·ªã·ªâƒ©√≤√≥·ªç·ªè√µ√¥·ªì·ªë·ªô·ªï·ªó∆°·ªù·ªõ·ª£·ªü·ª°√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª±·ª≠·ªØ·ª≥√Ω·ªµ·ª∑·ªπƒë]+\s+(?:l√†|c√≥|ƒë∆∞·ª£c|ƒë√£|s·∫Ω|ƒëang)\s+[a-z√†√°·∫°·∫£√£√¢·∫ß·∫•·∫≠·∫©·∫´ƒÉ·∫±·∫Ø·∫∑·∫≥·∫µ√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªá·ªÉ·ªÖ√¨√≠·ªã·ªâƒ©√≤√≥·ªç·ªè√µ√¥·ªì·ªë·ªô·ªï·ªó∆°·ªù·ªõ·ª£·ªü·ª°√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª±·ª≠·ªØ·ª≥√Ω·ªµ·ª∑·ªπƒë]+',
                # Classifiers
                r'\b(?:m·ªôt|hai|ba|b·ªën|nƒÉm)\s+(?:c√°i|con|chi·∫øc|ng∆∞·ªùi|quy·ªÉn|t·ªù|ly|chai|h·ªôp)\b',
                # Question particles
                r'\b(?:kh√¥ng|ch∆∞a|ƒë√¢u|g√¨|sao|th·∫ø\s+n√†o)\?',
                # Aspect markers
                r'\b(?:ƒë√£|ƒëang|s·∫Ω|v·ª´a|v·ª´a\s+m·ªõi|s·∫Øp|s·ª≠a)\s+[a-z√†√°·∫°·∫£√£√¢·∫ß·∫•·∫≠·∫©·∫´ƒÉ·∫±·∫Ø·∫∑·∫≥·∫µ√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªá·ªÉ·ªÖ√¨√≠·ªã·ªâƒ©√≤√≥·ªç·ªè√µ√¥·ªì·ªë·ªô·ªï·ªó∆°·ªù·ªõ·ª£·ªü·ª°√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª±·ª≠·ªØ·ª≥√Ω·ªµ·ª∑·ªπƒë]+'
            ],
            
            "english": [
                # Word order patterns (SVO)
                r'\b[A-Z][a-z]+\s+(?:is|are|was|were|has|have|had|will|would|can|could|should)\s+[a-z]+',
                # Plural markers
                r'\b[a-z]+s\b',
                # Past tense markers
                r'\b[a-z]+ed\b',
                # Progressive markers
                r'\b(?:is|are|was|were)\s+[a-z]+ing\b',
                # Question formation
                r'\b(?:What|Where|When|Why|How|Who|Which)\s+(?:is|are|was|were|do|does|did|will|would|can|could)\b'
            ]
        }
    
    def detect_language_comprehensive(self, text: str) -> LanguageSignature:
        """Comprehensive language detection using multiple modalities"""
        
        # Clean text for analysis
        cleaned_text = self._clean_text_for_analysis(text)
        
        # Multi-modal analysis
        character_analysis = self._analyze_character_patterns(cleaned_text)
        linguistic_analysis = self._analyze_linguistic_patterns(cleaned_text)
        cultural_analysis = self._analyze_cultural_patterns(cleaned_text)
        structure_analysis = self._analyze_script_structure(text)
        
        # Calculate composite scores
        language_scores = self._calculate_composite_scores(
            character_analysis, linguistic_analysis, cultural_analysis, structure_analysis
        )
        
        # Select best match
        if not language_scores:
            # Fallback
            return LanguageSignature(
                language="english",
                confidence=0.5,
                evidence=["fallback_detection"],
                character_patterns={},
                linguistic_patterns={},
                cultural_patterns={}
            )
        
        best_language = max(language_scores.items(), key=lambda x: x[1]["total_score"])
        language = best_language[0]
        score_data = best_language[1]
        
        # Build comprehensive signature
        signature = LanguageSignature(
            language=language,
            confidence=score_data["total_score"],
            evidence=score_data["evidence"],
            character_patterns=character_analysis.get(language, {}),
            linguistic_patterns=linguistic_analysis.get(language, {}),
            cultural_patterns=cultural_analysis.get(language, {})
        )
        
        return signature
    
    def _clean_text_for_analysis(self, text: str) -> str:
        """Clean text while preserving linguistic features"""
        # Remove script directions but keep dialogue and descriptions
        cleaned = re.sub(r'\b(?:EXT\.|INT\.|FADE|CUT|DISSOLVE)\b[^\n]*', '', text, flags=re.IGNORECASE)
        
        # Remove excessive whitespace
        cleaned = re.sub(r'\s+', ' ', cleaned)
        
        # Remove parentheticals in some cases but keep content
        cleaned = re.sub(r'\([^)]*\)', '', cleaned)
        
        return cleaned.strip()
    
    def _analyze_character_patterns(self, text: str) -> Dict[str, Dict[str, float]]:
        """Analyze character-based patterns"""
        results = {}
        total_chars = len(text)
        
        if total_chars == 0:
            return results
        
        for language, signature in self.language_signatures.items():
            scores = {}
            
            # Special character analysis
            special_chars = signature.get("special_chars", "")
            if special_chars:
                special_char_count = sum(1 for char in text if char in special_chars)
                scores["special_chars"] = (special_char_count / total_chars) * 100
            else:
                scores["special_chars"] = 0
            
            # Tone marker analysis
            tone_markers = signature.get("tone_markers", "")
            if tone_markers:
                tone_count = sum(1 for char in text if char in tone_markers)
                scores["tone_markers"] = (tone_count / total_chars) * 50
            else:
                scores["tone_markers"] = 0
            
            # Unique letter analysis
            unique_letters = signature.get("unique_letters", "")
            if unique_letters:
                unique_count = sum(1 for char in text if char in unique_letters)
                scores["unique_letters"] = (unique_count / total_chars) * 200
            else:
                scores["unique_letters"] = 0
            
            # Unicode range analysis
            char_ranges = signature.get("character_ranges", [])
            range_matches = 0
            for char in text:
                char_code = ord(char)
                for start, end in char_ranges:
                    if start <= char_code <= end:
                        range_matches += 1
                        break
            
            scores["unicode_ranges"] = (range_matches / total_chars) * 80 if total_chars > 0 else 0
            
            results[language] = scores
        
        return results
    
    def _analyze_linguistic_patterns(self, text: str) -> Dict[str, Dict[str, float]]:
        """Analyze linguistic structure patterns"""
        results = {}
        
        for language, patterns in self.linguistic_patterns.items():
            scores = {}
            total_score = 0
            
            for i, pattern in enumerate(patterns):
                matches = len(re.findall(pattern, text, re.IGNORECASE))
                pattern_score = matches * (5 - i)  # Weight earlier patterns higher
                scores[f"pattern_{i}"] = pattern_score
                total_score += pattern_score
            
            scores["total_linguistic"] = total_score
            results[language] = scores
        
        return results
    
    def _analyze_cultural_patterns(self, text: str) -> Dict[str, Dict[str, float]]:
        """Analyze cultural context patterns"""
        results = {}
        
        for language, patterns in self.cultural_indicators.items():
            scores = {}
            total_score = 0
            
            for i, pattern in enumerate(patterns):
                matches = len(re.findall(pattern, text, re.IGNORECASE))
                pattern_score = matches * 10  # Cultural indicators are strong signals
                scores[f"cultural_{i}"] = pattern_score
                total_score += pattern_score
            
            scores["total_cultural"] = total_score
            results[language] = scores
        
        return results
    
    def _analyze_script_structure(self, text: str) -> Dict[str, float]:
        """Analyze script structure for language clues"""
        structure_scores = {}
        
        # Analyze dialogue structure
        vietnamese_dialogue = len(re.findall(r'([A-Z√Ä√Å·∫†·∫¢√É√Ç·∫¶·∫§·∫¨·∫®·∫™ƒÇ·∫∞·∫Æ·∫∂·∫≤·∫¥√à√â·∫∏·∫∫·∫º√ä·ªÄ·∫æ·ªÜ·ªÇ·ªÑ√å√ç·ªä·ªàƒ®√í√ì·ªå·ªé√ï√î·ªí·ªê·ªò·ªî·ªñ∆†·ªú·ªö·ª¢·ªû·ª†√ô√ö·ª§·ª¶≈®∆Ø·ª™·ª®·ª∞·ª¨·ªÆ·ª≤√ù·ª¥·ª∂·ª∏ƒê][A-Za-z√†√°·∫°·∫£√£√¢·∫ß·∫•·∫≠·∫©·∫´ƒÉ·∫±·∫Ø·∫∑·∫≥·∫µ√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªá·ªÉ·ªÖ√¨√≠·ªã·ªâƒ©√≤√≥·ªç·ªè√µ√¥·ªì·ªë·ªô·ªï·ªó∆°·ªù·ªõ·ª£·ªü·ª°√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª±·ª≠·ªØ·ª≥√Ω·ªµ·ª∑·ªπƒë]+)\s*\n', text, re.MULTILINE))
        english_dialogue = len(re.findall(r'([A-Z][A-Za-z]+)\s*\n', text, re.MULTILINE))
        
        structure_scores["vietnamese"] = vietnamese_dialogue * 5
        structure_scores["english"] = english_dialogue * 3
        
        # Analyze scene directions
        vietnamese_directions = len(re.findall(r'(c·∫£nh|chuy·ªÉn|fade|cut)', text, re.IGNORECASE))
        english_directions = len(re.findall(r'(scene|fade|cut|ext|int)', text, re.IGNORECASE))
        
        structure_scores["vietnamese"] = structure_scores.get("vietnamese", 0) + vietnamese_directions * 2
        structure_scores["english"] = structure_scores.get("english", 0) + english_directions * 2
        
        return structure_scores
    
    def _calculate_composite_scores(self, character_analysis: Dict, linguistic_analysis: Dict, 
                                  cultural_analysis: Dict, structure_analysis: Dict) -> Dict[str, Dict]:
        """Calculate composite scores for each language"""
        
        composite_scores = {}
        
        # Get all possible languages
        all_languages = set()
        all_languages.update(character_analysis.keys())
        all_languages.update(linguistic_analysis.keys())
        all_languages.update(cultural_analysis.keys())
        all_languages.update(structure_analysis.keys())
        
        for language in all_languages:
            evidence = []
            
            # Character score (weight: 30%)
            char_scores = character_analysis.get(language, {})
            char_total = sum(char_scores.values())
            char_weighted = char_total * 0.3
            
            if char_total > 0:
                evidence.append(f"Character patterns: {char_total:.1f}")
            
            # Linguistic score (weight: 25%)
            ling_scores = linguistic_analysis.get(language, {})
            ling_total = ling_scores.get("total_linguistic", 0)
            ling_weighted = ling_total * 0.25
            
            if ling_total > 0:
                evidence.append(f"Linguistic patterns: {ling_total:.1f}")
            
            # Cultural score (weight: 25%)
            cult_scores = cultural_analysis.get(language, {})
            cult_total = cult_scores.get("total_cultural", 0)
            cult_weighted = cult_total * 0.25
            
            if cult_total > 0:
                evidence.append(f"Cultural indicators: {cult_total:.1f}")
            
            # Structure score (weight: 20%)
            struct_score = structure_analysis.get(language, 0)
            struct_weighted = struct_score * 0.2
            
            if struct_score > 0:
                evidence.append(f"Script structure: {struct_score:.1f}")
            
            # Total composite score
            total_score = char_weighted + ling_weighted + cult_weighted + struct_weighted
            
            # Normalize to 0-1 range
            normalized_score = min(1.0, total_score / 100.0)
            
            composite_scores[language] = {
                "total_score": normalized_score,
                "character_score": char_weighted,
                "linguistic_score": ling_weighted,
                "cultural_score": cult_weighted,
                "structure_score": struct_weighted,
                "evidence": evidence
            }
        
        return composite_scores

# Test enhanced language detection
if __name__ == "__main__":
    detector = EnhancedLanguageDetection()
    
    test_texts = [
        """
        FADE IN:
        
        EXT. NH√Ä VI·ªÜT NAM - S√ÅNG S·ªöM
        
        Anh MINH (40 tu·ªïi, ƒë√†n √¥ng Vi·ªát Nam, d√°ng g·∫ßy, da ngƒÉm ƒëen) b∆∞·ªõc ra kh·ªèi nh√† tranh nh·ªè.
        
        MINH
        Ch√†o em Lan! S√°ng nay em d·∫≠y s·ªõm qu√°!
        
        LAN
        Ch√†o anh Minh! Em ph·∫£i h√°i rau s·ªõm ƒë·ªÉ mang ra ch·ª£ b√°n.
        """,
        
        """
        FADE IN:
        
        EXT. AMERICAN HOUSE - MORNING
        
        JOHN (40 years old, tall American man, thin build) walks out of his small house.
        
        JOHN
        Good morning, Sarah! You're up early today!
        
        SARAH
        Good morning, John! I have to pick vegetables early to take them to market.
        """
    ]
    
    print("üß† TESTING ENHANCED LANGUAGE DETECTION:")
    print("="*60)
    
    for i, text in enumerate(test_texts):
        print(f"\nüîç Test {i+1}:")
        signature = detector.detect_language_comprehensive(text)
        
        print(f"   Language: {signature.language}")
        print(f"   Confidence: {signature.confidence:.3f}")
        print(f"   Evidence: {signature.evidence}")
        print(f"   Character patterns: {sum(signature.character_patterns.values()):.1f}")
        print(f"   Linguistic patterns: {sum(signature.linguistic_patterns.values()):.1f}")
        print(f"   Cultural patterns: {sum(signature.cultural_patterns.values()):.1f}")
    
    print(f"\n‚úÖ ENHANCED LANGUAGE DETECTION READY! üß†üéØ")
