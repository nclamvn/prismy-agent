"""
PRISM Enhanced Scene Analyzer
Ultra-sophisticated scene detection with cinematic intelligence
"""

import re
import json
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass, field
from enum import Enum
import time

class SceneType(Enum):
    DIALOGUE = "dialogue"
    ACTION = "action"
    TRANSITION = "transition"
    ESTABLISHING = "establishing"
    CLOSE_UP = "close_up"
    WIDE_SHOT = "wide_shot"
    MONTAGE = "montage"

class CinematicElement(Enum):
    FADE_IN = "fade_in"
    FADE_OUT = "fade_out"
    CUT_TO = "cut_to"
    DISSOLVE = "dissolve"
    MATCH_CUT = "match_cut"
    CLOSE_UP = "close_up"
    WIDE_SHOT = "wide_shot"
    MEDIUM_SHOT = "medium_shot"

@dataclass
class EnhancedVideoScene:
    """Enhanced video scene with cinematic intelligence"""
    id: int
    content: str
    scene_type: SceneType
    duration: float
    characters: List[str] = field(default_factory=list)
    setting: str = ""
    actions: List[str] = field(default_factory=list)
    dialogue: Optional[str] = None
    
    # Enhanced cinematic features
    cinematic_elements: List[CinematicElement] = field(default_factory=list)
    camera_angles: List[str] = field(default_factory=list)
    emotional_tone: str = ""
    pacing: str = "moderate"  # slow, moderate, fast, varied
    visual_complexity: float = 0.5  # 0.0-1.0
    
    # Technical specifications
    recommended_shots: List[str] = field(default_factory=list)
    lighting_requirements: str = ""
    sound_design: str = ""
    
    # Continuity markers
    continuity_markers: Dict[str, str] = field(default_factory=dict)

class EnhancedSceneAnalyzer:
    """
    Ultra-sophisticated scene analysis with cinematic intelligence
    Professional film industry standards
    """
    
    def __init__(self, target_duration: float = 8.0, language: str = "auto"):
        self.target_duration = target_duration
        self.min_duration = 4.0
        self.max_duration = 15.0
        self.language = language
        
        # Cinematic intelligence patterns
        self.cinematic_patterns = self._build_cinematic_patterns()
        self.pacing_indicators = self._build_pacing_indicators()
        self.emotional_indicators = self._build_emotional_indicators()
        
    def _build_cinematic_patterns(self) -> Dict[str, Dict[str, List[str]]]:
        """Build comprehensive cinematic pattern recognition"""
        return {
            "vietnamese": {
                "scene_transitions": [
                    r'(?i)(fade\s+in|fade\s+out|cut\s+to|dissolve|chuyá»ƒn\s+cáº£nh)',
                    r'(?i)(cáº¯t\s+Ä‘áº¿n|chuyá»ƒn\s+sang|fade|cut)',
                    r'(?i)(sau\s+Ä‘Ã³|tiáº¿p\s+theo|lÃºc\s+nÃ y|Ä‘á»“ng\s+thá»i)',
                    r'(?i)(ext\.|int\.|ngoáº¡i\s+cáº£nh|ná»™i\s+cáº£nh)'
                ],
                "camera_directions": [
                    r'(?i)(close\s+up|wide\s+shot|medium\s+shot)',
                    r'(?i)(cáº­n\s+cáº£nh|toÃ n\s+cáº£nh|gÃ³c\s+rá»™ng|gÃ³c\s+háº¹p)',
                    r'(?i)(camera\s+.*?|mÃ¡y\s+quay|gÃ³c\s+quay)',
                    r'(?i)(tá»«\s+trÃªn\s+xuá»‘ng|tá»«\s+dÆ°á»›i\s+lÃªn|ngang\s+máº¯t)'
                ],
                "dialogue_markers": [
                    r'([A-ZÃ€Ãáº áº¢ÃƒÃ‚áº¦áº¤áº¬áº¨áºªÄ‚áº°áº®áº¶áº²áº´ÃˆÃ‰áº¸áººáº¼ÃŠá»€áº¾á»†á»‚á»„ÃŒÃá»Šá»ˆÄ¨Ã’Ã“á»Œá»Ã•Ã”á»’á»á»˜á»”á»–Æ á»œá»šá»¢á»á» Ã™Ãšá»¤á»¦Å¨Æ¯á»ªá»¨á»°á»¬á»®á»²Ãá»´á»¶á»¸Ä][A-Za-zÃ Ã¡áº¡áº£Ã£Ã¢áº§áº¥áº­áº©áº«Äƒáº±áº¯áº·áº³áºµÃ¨Ã©áº¹áº»áº½Ãªá»áº¿á»‡á»ƒá»…Ã¬Ã­á»‹á»‰Ä©Ã²Ã³á»á»ÃµÃ´á»“á»‘á»™á»•á»—Æ¡á»á»›á»£á»Ÿá»¡Ã¹Ãºá»¥á»§Å©Æ°á»«á»©á»±á»­á»¯á»³Ã½á»µá»·á»¹Ä‘]+)\s*\n.*?"([^"]*)"',
                    r'([A-ZÃ€Ãáº áº¢ÃƒÃ‚áº¦áº¤áº¬áº¨áºªÄ‚áº°áº®áº¶áº²áº´ÃˆÃ‰áº¸áººáº¼ÃŠá»€áº¾á»†á»‚á»„ÃŒÃá»Šá»ˆÄ¨Ã’Ã“á»Œá»Ã•Ã”á»’á»á»˜á»”á»–Æ á»œá»šá»¢á»á» Ã™Ãšá»¤á»¦Å¨Æ¯á»ªá»¨á»°á»¬á»®á»²Ãá»´á»¶á»¸Ä][A-Za-zÃ Ã¡áº¡áº£Ã£Ã¢áº§áº¥áº­áº©áº«Äƒáº±áº¯áº·áº³áºµÃ¨Ã©áº¹áº»áº½Ãªá»áº¿á»‡á»ƒá»…Ã¬Ã­á»‹á»‰Ä©Ã²Ã³á»á»ÃµÃ´á»“á»‘á»™á»•á»—Æ¡á»á»›á»£á»Ÿá»¡Ã¹Ãºá»¥á»§Å©Æ°á»«á»©á»±á»­á»¯á»³Ã½á»µá»·á»¹Ä‘]+)\s*:',
                    r'(nÃ³i|há»i|thÃ¬\s+tháº§m|la\s+lÃªn|thá»‘t\s+lÃªn)'
                ],
                "action_sequences": [
                    r'(cháº¡y|nháº£y|Ä‘Ã¡nh|báº¯n|lÃ¡i\s+xe|bay|rÆ¡i|ngÃ£)',
                    r'(nhanh\s+chÃ³ng|tá»«\s+tá»«|Ä‘á»™t\s+ngá»™t|bá»—ng\s+nhiÃªn)',
                    r'(cÃ¹ng\s+lÃºc|Ä‘á»“ng\s+thá»i|trong\s+khi|liá»n\s+sau)'
                ]
            },
            "english": {
                "scene_transitions": [
                    r'(?i)(fade\s+in|fade\s+out|cut\s+to|dissolve|transition)',
                    r'(?i)(ext\.|int\.|exterior|interior)',
                    r'(?i)(meanwhile|later|then|next|suddenly)'
                ],
                "camera_directions": [
                    r'(?i)(close\s+up|wide\s+shot|medium\s+shot|extreme)',
                    r'(?i)(camera\s+.*?|angle|pov|point\s+of\s+view)',
                    r'(?i)(overhead|low\s+angle|high\s+angle|eye\s+level)'
                ],
                "dialogue_markers": [
                    r'([A-Z][A-Za-z]+)\s*\n.*?"([^"]*)"',
                    r'([A-Z][A-Za-z]+)\s*:',
                    r'(says|asks|whispers|shouts|exclaims)'
                ],
                "action_sequences": [
                    r'(runs|jumps|fights|shoots|drives|flies|falls)',
                    r'(quickly|slowly|suddenly|gradually)',
                    r'(simultaneously|meanwhile|during|while)'
                ]
            }
        }
    
    def _build_pacing_indicators(self) -> Dict[str, List[str]]:
        """Build pacing detection patterns"""
        return {
            "fast": [
                r'(nhanh|rapid|quick|fast|sudden|immediate)',
                r'(cháº¡y|nháº£y|Ä‘Ã¡nh|báº¯n|run|jump|fight|chase)',
                r'(cáº¥p\s+tá»‘c|gáº¥p\s+rÃºt|vá»™i\s+vÃ£|urgent|rush)'
            ],
            "slow": [
                r'(cháº­m|slow|gradual|gentle|peaceful)',
                r'(tá»«\s+tá»«|nháº¹\s+nhÃ ng|slowly|gently)',
                r'(yÃªn\s+bÃ¬nh|tÄ©nh\s+láº·ng|calm|quiet|serene)'
            ],
            "varied": [
                r'(thay\s+Ä‘á»•i|biáº¿n\s+thiÃªn|varies|changes)',
                r'(tá»«.*?Ä‘áº¿n|from.*?to)',
                r'(lÃºc.*?lÃºc|sometimes.*?sometimes)'
            ]
        }
    
    def _build_emotional_indicators(self) -> Dict[str, List[str]]:
        """Build emotional tone detection"""
        return {
            "joyful": [r'(vui|cÆ°á»i|háº¡nh\s+phÃºc|happy|joy|laugh|smile)'],
            "sad": [r'(buá»“n|khÃ³c|Ä‘au\s+khá»•|sad|cry|sorrow|grief)'],
            "tense": [r'(cÄƒng\s+tháº³ng|lo\s+láº¯ng|tense|anxious|worried)'],
            "romantic": [r'(lÃ£ng\s+máº¡n|yÃªu|tÃ¬nh\s+cáº£m|romantic|love|tender)'],
            "mysterious": [r'(bÃ­\s+áº©n|ká»³\s+láº¡|mysterious|strange|eerie)'],
            "action": [r'(hÃ nh\s+Ä‘á»™ng|phiÃªu\s+lÆ°u|action|adventure|thrill)']
        }
    
    def analyze_script_enhanced(self, script: str) -> List[EnhancedVideoScene]:
        """
        Enhanced script analysis with cinematic intelligence
        """
        
        print("ğŸ¬ Enhanced Scene Analysis Starting...")
        start_time = time.time()
        
        # Detect language
        language = self._detect_language_advanced(script)
        patterns = self.cinematic_patterns.get(language, self.cinematic_patterns["english"])
        
        # Step 1: Intelligent scene segmentation
        raw_segments = self._intelligent_scene_segmentation(script, patterns)
        print(f"   ğŸ“‹ Initial segmentation: {len(raw_segments)} segments")
        
        # Step 2: Cinematic analysis
        analyzed_segments = []
        for i, segment in enumerate(raw_segments):
            analyzed_segment = self._analyze_segment_cinematics(segment, i + 1, patterns, language)
            analyzed_segments.append(analyzed_segment)
        
        # Step 3: Duration optimization
        optimized_scenes = self._optimize_scene_durations(analyzed_segments)
        print(f"   â±ï¸  Duration optimization: {len(optimized_scenes)} final scenes")
        
        # Step 4: Continuity analysis
        continuity_enhanced_scenes = self._analyze_continuity(optimized_scenes)
        
        processing_time = time.time() - start_time
        print(f"   âœ… Enhanced analysis complete: {processing_time:.2f}s")
        
        return continuity_enhanced_scenes
    
    def _detect_language_advanced(self, script: str) -> str:
        """Advanced language detection with multiple indicators"""
        
        # Character-based detection
        vietnamese_chars = len(re.findall(r'[Ã Ã¡áº¡áº£Ã£Ã¢áº§áº¥áº­áº©áº«Äƒáº±áº¯áº·áº³áºµÃ¨Ã©áº¹áº»áº½Ãªá»áº¿á»‡á»ƒá»…Ã¬Ã­á»‹á»‰Ä©Ã²Ã³á»á»ÃµÃ´á»“á»‘á»™á»•á»—Æ¡á»á»›á»£á»Ÿá»¡Ã¹Ãºá»¥á»§Å©Æ°á»«á»©á»±á»­á»¯á»³Ã½á»µá»·á»¹Ä‘]', script.lower()))
        total_chars = len(re.findall(r'[a-zA-ZÃ Ã¡áº¡áº£Ã£Ã¢áº§áº¥áº­áº©áº«Äƒáº±áº¯áº·áº³áºµÃ¨Ã©áº¹áº»áº½Ãªá»áº¿á»‡á»ƒá»…Ã¬Ã­á»‹á»‰Ä©Ã²Ã³á»á»ÃµÃ´á»“á»‘á»™á»•á»—Æ¡á»á»›á»£á»Ÿá»¡Ã¹Ãºá»¥á»§Å©Æ°á»«á»©á»±á»­á»¯á»³Ã½á»µá»·á»¹Ä‘]', script.lower()))
        
        # Word-based detection
        vietnamese_keywords = len(re.findall(r'\b(?:anh|em|chá»‹|tuá»•i|sÃ¡ng|chiá»u|tá»‘i|nhÃ |Ä‘Æ°á»ng|nÃ³i|há»i)\b', script.lower()))
        english_keywords = len(re.findall(r'\b(?:the|and|is|are|said|asked|house|street|morning|evening)\b', script.lower()))
        
        # Structure-based detection
        vietnamese_titles = len(re.findall(r'\b(?:anh|em|chá»‹|Ã´ng|bÃ )\s+[A-ZÃ€Ãáº áº¢ÃƒÃ‚áº¦áº¤áº¬áº¨áºªÄ‚áº°áº®áº¶áº²áº´ÃˆÃ‰áº¸áººáº¼ÃŠá»€áº¾á»†á»‚á»„ÃŒÃá»Šá»ˆÄ¨Ã’Ã“á»Œá»Ã•Ã”á»’á»á»˜á»”á»–Æ á»œá»šá»¢á»á» Ã™Ãšá»¤á»¦Å¨Æ¯á»ªá»¨á»°á»¬á»®á»²Ãá»´á»¶á»¸Ä]', script))
        
        # Scoring
        vietnamese_score = 0
        english_score = 0
        
        if total_chars > 0:
            vietnamese_score += (vietnamese_chars / total_chars) * 40
        
        vietnamese_score += vietnamese_keywords * 5
        vietnamese_score += vietnamese_titles * 3
        english_score += english_keywords * 3
        
        return "vietnamese" if vietnamese_score > english_score else "english"
    
    def _intelligent_scene_segmentation(self, script: str, patterns: Dict) -> List[str]:
        """Intelligent scene segmentation using multiple criteria"""
        
        # Find all potential break points
        break_points = [0]
        
        # Cinematic transitions (highest priority)
        for pattern in patterns["scene_transitions"]:
            matches = list(re.finditer(pattern, script, re.MULTILINE | re.IGNORECASE))
            for match in matches:
                break_points.append(match.start())
        
        # Dialogue changes (medium priority)
        dialogue_pattern = r'^([A-ZÃ€Ãáº áº¢ÃƒÃ‚áº¦áº¤áº¬áº¨áºªÄ‚áº°áº®áº¶áº²áº´ÃˆÃ‰áº¸áººáº¼ÃŠá»€áº¾á»†á»‚á»„ÃŒÃá»Šá»ˆÄ¨Ã’Ã“á»Œá»Ã•Ã”á»’á»á»˜á»”á»–Æ á»œá»šá»¢á»á» Ã™Ãšá»¤á»¦Å¨Æ¯á»ªá»¨á»°á»¬á»®á»²Ãá»´á»¶á»¸Ä][A-Za-zÃ Ã¡áº¡áº£Ã£Ã¢áº§áº¥áº­áº©áº«Äƒáº±áº¯áº·áº³áºµÃ¨Ã©áº¹áº»áº½Ãªá»áº¿á»‡á»ƒá»…Ã¬Ã­á»‹á»‰Ä©Ã²Ã³á»á»ÃµÃ´á»“á»‘á»™á»•á»—Æ¡á»á»›á»£á»Ÿá»¡Ã¹Ãºá»¥á»§Å©Æ°á»«á»©á»±á»­á»¯á»³Ã½á»µá»·á»¹Ä‘]+)\s*$'
        matches = list(re.finditer(dialogue_pattern, script, re.MULTILINE))
        for match in matches:
            break_points.append(match.start())
        
        # Natural breaks (paragraph breaks, etc.)
        paragraph_breaks = [m.start() for m in re.finditer(r'\n\s*\n', script)]
        break_points.extend(paragraph_breaks)
        
        # Sort and deduplicate
        break_points = sorted(list(set(break_points)))
        break_points.append(len(script))
        
        # Extract segments
        segments = []
        for i in range(len(break_points) - 1):
            start = break_points[i]
            end = break_points[i + 1]
            segment = script[start:end].strip()
            if segment and len(segment) > 10:  # Minimum segment length
                segments.append(segment)
        
        return segments
    
    def _analyze_segment_cinematics(self, segment: str, segment_id: int, 
                                  patterns: Dict, language: str) -> EnhancedVideoScene:
        """Analyze cinematic elements of a segment"""
        
        # Basic analysis
        scene_type = self._classify_scene_type_advanced(segment, patterns)
        duration = self._estimate_duration_advanced(segment, scene_type)
        
        # Extract cinematic elements
        cinematic_elements = self._extract_cinematic_elements(segment, patterns)
        camera_angles = self._extract_camera_directions(segment, patterns)
        
        # Analyze emotional tone
        emotional_tone = self._analyze_emotional_tone(segment)
        
        # Determine pacing
        pacing = self._analyze_pacing(segment)
        
        # Calculate visual complexity
        visual_complexity = self._calculate_visual_complexity(segment)
        
        # Extract dialogue and actions
        dialogue = self._extract_dialogue_advanced(segment)
        actions = self._extract_actions_advanced(segment)
        
        # Generate recommendations
        recommended_shots = self._generate_shot_recommendations(segment, scene_type)
        lighting_requirements = self._analyze_lighting_requirements(segment)
        sound_design = self._analyze_sound_requirements(segment)
        
        return EnhancedVideoScene(
            id=segment_id,
            content=segment,
            scene_type=scene_type,
            duration=duration,
            cinematic_elements=cinematic_elements,
            camera_angles=camera_angles,
            emotional_tone=emotional_tone,
            pacing=pacing,
            visual_complexity=visual_complexity,
            dialogue=dialogue,
            actions=actions,
            recommended_shots=recommended_shots,
            lighting_requirements=lighting_requirements,
            sound_design=sound_design
        )
    
    def _classify_scene_type_advanced(self, segment: str, patterns: Dict) -> SceneType:
        """Advanced scene type classification"""
        
        # Check for dialogue
        dialogue_indicators = 0
        for pattern in patterns["dialogue_markers"]:
            if re.search(pattern, segment, re.IGNORECASE):
                dialogue_indicators += 1
        
        # Check for action
        action_indicators = 0
        for pattern in patterns["action_sequences"]:
            if re.search(pattern, segment, re.IGNORECASE):
                action_indicators += 1
        
        # Check for transitions
        transition_indicators = 0
        for pattern in patterns["scene_transitions"]:
            if re.search(pattern, segment, re.IGNORECASE):
                transition_indicators += 1
        
        # Check for camera directions
        camera_indicators = 0
        for pattern in patterns["camera_directions"]:
            if re.search(pattern, segment, re.IGNORECASE):
                camera_indicators += 1
        
        # Classification logic
        if transition_indicators > 0:
            return SceneType.TRANSITION
        elif dialogue_indicators >= 2:
            return SceneType.DIALOGUE
        elif action_indicators >= 2:
            return SceneType.ACTION
        elif camera_indicators > 0:
            if re.search(r'(?i)(close\s+up|cáº­n\s+cáº£nh)', segment):
                return SceneType.CLOSE_UP
            elif re.search(r'(?i)(wide\s+shot|toÃ n\s+cáº£nh)', segment):
                return SceneType.WIDE_SHOT
        else:
            return SceneType.ESTABLISHING
    
    def _estimate_duration_advanced(self, segment: str, scene_type: SceneType) -> float:
        """Advanced duration estimation based on content analysis"""
        
        # Base duration calculation
        word_count = len(segment.split())
        
        # Adjust for scene type
        if scene_type == SceneType.DIALOGUE:
            base_wps = 2.0  # words per second for dialogue
        elif scene_type == SceneType.ACTION:
            base_wps = 3.5  # faster for action
        elif scene_type == SceneType.TRANSITION:
            base_wps = 4.0  # very fast for transitions
        else:
            base_wps = 2.5  # standard
        
        # Adjust for content complexity
        complexity_factors = {
            "character_count": len(re.findall(r'[A-ZÃ€Ãáº áº¢ÃƒÃ‚áº¦áº¤áº¬áº¨áºªÄ‚áº°áº®áº¶áº²áº´ÃˆÃ‰áº¸áººáº¼ÃŠá»€áº¾á»†á»‚á»„ÃŒÃá»Šá»ˆÄ¨Ã’Ã“á»Œá»Ã•Ã”á»’á»á»˜á»”á»–Æ á»œá»šá»¢á»á» Ã™Ãšá»¤á»¦Å¨Æ¯á»ªá»¨á»°á»¬á»®á»²Ãá»´á»¶á»¸Ä][a-zA-ZÃ Ã¡áº¡áº£Ã£Ã¢áº§áº¥áº­áº©áº«Äƒáº±áº¯áº·áº³áºµÃ¨Ã©áº¹áº»áº½Ãªá»áº¿á»‡á»ƒá»…Ã¬Ã­á»‹á»‰Ä©Ã²Ã³á»á»ÃµÃ´á»“á»‘á»™á»•á»—Æ¡á»á»›á»£á»Ÿá»¡Ã¹Ãºá»¥á»§Å©Æ°á»«á»©á»±á»­á»¯á»³Ã½á»µá»·á»¹Ä‘]+', segment)) * 0.5,
            "dialogue_count": len(re.findall(r'"[^"]*"', segment)) * 1.0,
            "action_count": len(re.findall(r'(cháº¡y|nháº£y|Ä‘Ã¡nh|run|jump|fight)', segment, re.IGNORECASE)) * 0.3
        }
        
        complexity_adjustment = sum(complexity_factors.values()) / 10.0
        adjusted_wps = base_wps - complexity_adjustment
        
        estimated_duration = word_count / max(1.0, adjusted_wps)
        
        # Clamp to reasonable bounds
        return max(self.min_duration, min(self.max_duration, estimated_duration))
    
    def _extract_cinematic_elements(self, segment: str, patterns: Dict) -> List[CinematicElement]:
        """Extract cinematic elements from segment"""
        elements = []
        
        cinematic_mapping = {
            r'(?i)fade\s+in': CinematicElement.FADE_IN,
            r'(?i)fade\s+out': CinematicElement.FADE_OUT,
            r'(?i)cut\s+to': CinematicElement.CUT_TO,
            r'(?i)dissolve': CinematicElement.DISSOLVE,
            r'(?i)close\s+up': CinematicElement.CLOSE_UP,
            r'(?i)wide\s+shot': CinematicElement.WIDE_SHOT,
            r'(?i)medium\s+shot': CinematicElement.MEDIUM_SHOT
        }
        
        for pattern, element in cinematic_mapping.items():
            if re.search(pattern, segment):
                elements.append(element)
        
        return elements
    
    def _extract_camera_directions(self, segment: str, patterns: Dict) -> List[str]:
        """Extract camera direction information"""
        directions = []
        
        for pattern in patterns["camera_directions"]:
            matches = re.findall(pattern, segment, re.IGNORECASE)
            directions.extend(matches)
        
        return list(set(directions))
    
    def _analyze_emotional_tone(self, segment: str) -> str:
        """Analyze emotional tone of segment"""
        emotion_scores = {}
        
        for emotion, patterns in self.emotional_indicators.items():
            score = 0
            for pattern in patterns:
                matches = len(re.findall(pattern, segment, re.IGNORECASE))
                score += matches
            emotion_scores[emotion] = score
        
        if not emotion_scores or max(emotion_scores.values()) == 0:
            return "neutral"
        
        return max(emotion_scores.items(), key=lambda x: x[1])[0]
    
    def _analyze_pacing(self, segment: str) -> str:
        """Analyze pacing of segment"""
        pacing_scores = {}
        
        for pacing_type, patterns in self.pacing_indicators.items():
            score = 0
            for pattern in patterns:
                matches = len(re.findall(pattern, segment, re.IGNORECASE))
                score += matches
            pacing_scores[pacing_type] = score
        
        if not pacing_scores or max(pacing_scores.values()) == 0:
            return "moderate"
        
        return max(pacing_scores.items(), key=lambda x: x[1])[0]
    
    def _calculate_visual_complexity(self, segment: str) -> float:
        """Calculate visual complexity score"""
        complexity_factors = {
            "character_count": len(re.findall(r'[A-ZÃ€Ãáº áº¢ÃƒÃ‚áº¦áº¤áº¬áº¨áºªÄ‚áº°áº®áº¶áº²áº´ÃˆÃ‰áº¸áººáº¼ÃŠá»€áº¾á»†á»‚á»„ÃŒÃá»Šá»ˆÄ¨Ã’Ã“á»Œá»Ã•Ã”á»’á»á»˜á»”á»–Æ á»œá»šá»¢á»á» Ã™Ãšá»¤á»¦Å¨Æ¯á»ªá»¨á»°á»¬á»®á»²Ãá»´á»¶á»¸Ä][a-zA-ZÃ Ã¡áº¡áº£Ã£Ã¢áº§áº¥áº­áº©áº«Äƒáº±áº¯áº·áº³áºµÃ¨Ã©áº¹áº»áº½Ãªá»áº¿á»‡á»ƒá»…Ã¬Ã­á»‹á»‰Ä©Ã²Ã³á»á»ÃµÃ´á»“á»‘á»™á»•á»—Æ¡á»á»›á»£á»Ÿá»¡Ã¹Ãºá»¥á»§Å©Æ°á»«á»©á»±á»­á»¯á»³Ã½á»µá»·á»¹Ä‘]+', segment)) / 10.0,
            "action_density": len(re.findall(r'(cháº¡y|nháº£y|Ä‘Ã¡nh|báº¯n|run|jump|fight|shoot)', segment, re.IGNORECASE)) / 5.0,
            "location_changes": len(re.findall(r'(cut\s+to|chuyá»ƒn|fade)', segment, re.IGNORECASE)) / 3.0,
            "dialogue_complexity": len(re.findall(r'"[^"]{20,}"', segment)) / 5.0
        }
        
        total_complexity = sum(complexity_factors.values())
        return min(1.0, total_complexity)
    
    def _extract_dialogue_advanced(self, segment: str) -> Optional[str]:
        """Advanced dialogue extraction"""
        dialogue_patterns = [
            r'"([^"]+)"',
            r'"([^"]+)"',
            r'([A-ZÃ€Ãáº áº¢ÃƒÃ‚áº¦áº¤áº¬áº¨áºªÄ‚áº°áº®áº¶áº²áº´ÃˆÃ‰áº¸áººáº¼ÃŠá»€áº¾á»†á»‚á»„ÃŒÃá»Šá»ˆÄ¨Ã’Ã“á»Œá»Ã•Ã”á»’á»á»˜á»”á»–Æ á»œá»šá»¢á»á» Ã™Ãšá»¤á»¦Å¨Æ¯á»ªá»¨á»°á»¬á»®á»²Ãá»´á»¶á»¸Ä][a-zA-ZÃ Ã¡áº¡áº£Ã£Ã¢áº§áº¥áº­áº©áº«Äƒáº±áº¯áº·áº³áºµÃ¨Ã©áº¹áº»áº½Ãªá»áº¿á»‡á»ƒá»…Ã¬Ã­á»‹á»‰Ä©Ã²Ã³á»á»ÃµÃ´á»“á»‘á»™á»•á»—Æ¡á»á»›á»£á»Ÿá»¡Ã¹Ãºá»¥á»§Å©Æ°á»«á»©á»±á»­á»¯á»³Ã½á»µá»·á»¹Ä‘]+)\s*:\s*([^.\n]+)'
        ]
        
        for pattern in dialogue_patterns:
            matches = re.findall(pattern, segment, re.IGNORECASE)
            if matches:
                return matches[0] if isinstance(matches[0], str) else matches[0][1]
        
        return None
    
    def _extract_actions_advanced(self, segment: str) -> List[str]:
        """Advanced action extraction"""
        action_patterns = [
            r'(bÆ°á»›c\s+ra|Ä‘i\s+bá»™|cháº¡y\s+nhanh|nháº£y\s+lÃªn)',
            r'(nhÃ¬n|nghe|cÆ°á»i|khÃ³c|nÃ³i|há»i)',
            r'(walks|runs|jumps|looks|listens|smiles|cries|says|asks)'
        ]
        
        actions = []
        for pattern in action_patterns:
            matches = re.findall(pattern, segment, re.IGNORECASE)
            actions.extend(matches)
        
        return list(set(actions))
    
    def _generate_shot_recommendations(self, segment: str, scene_type: SceneType) -> List[str]:
        """Generate cinematography recommendations"""
        recommendations = []
        
        # Base recommendations by scene type
        if scene_type == SceneType.DIALOGUE:
            recommendations.extend(["Medium shot", "Over-shoulder shots", "Close-up for emotions"])
        elif scene_type == SceneType.ACTION:
            recommendations.extend(["Wide shot for context", "Dynamic camera movement", "Quick cuts"])
        elif scene_type == SceneType.ESTABLISHING:
            recommendations.extend(["Wide establishing shot", "Slow camera movement", "Environmental details"])
        
        # Context-specific recommendations
        if re.search(r'(cáº­n\s+cáº£nh|close\s+up)', segment, re.IGNORECASE):
            recommendations.append("Extreme close-up")
        
        if re.search(r'(cháº¡y|nháº£y|run|jump)', segment, re.IGNORECASE):
            recommendations.append("Tracking shot")
        
        return recommendations
    
    def _analyze_lighting_requirements(self, segment: str) -> str:
        """Analyze lighting requirements"""
        lighting_indicators = {
            "natural": r'(Ã¡nh\s+náº¯ng|sunlight|daylight|sÃ¡ng\s+sá»›m|morning)',
            "dramatic": r'(bÃ³ng\s+tá»‘i|shadow|dramatic|cÄƒng\s+tháº³ng|tense)',
            "soft": r'(dá»‹u\s+nháº¹|soft|gentle|áº¥m\s+Ã¡p|warm)',
            "bright": r'(sÃ¡ng\s+rá»±c|bright|brilliant|rá»±c\s+rá»¡)',
            "dim": r'(má»\s+áº£o|dim|low\s+light|tá»‘i)'
        }
        
        for lighting_type, pattern in lighting_indicators.items():
            if re.search(pattern, segment, re.IGNORECASE):
                return lighting_type
        
        return "natural"
    
    def _analyze_sound_requirements(self, segment: str) -> str:
        """Analyze sound design requirements"""
        sound_indicators = {
            "ambient": r'(yÃªn\s+tÄ©nh|quiet|peaceful|ambient)',
            "dynamic": r'(á»“n\s+Ã o|loud|dynamic|action)',
            "dialogue": r'(nÃ³i\s+chuyá»‡n|conversation|dialogue|talk)',
            "music": r'(nháº¡c|music|melody|rhythm)',
            "effects": r'(tiáº¿ng|sound\s+of|noise|effect)'
        }
        
        for sound_type, pattern in sound_indicators.items():
            if re.search(pattern, segment, re.IGNORECASE):
                return sound_type
        
        return "ambient"
    
    def _optimize_scene_durations(self, scenes: List[EnhancedVideoScene]) -> List[EnhancedVideoScene]:
        """Optimize scene durations for target length"""
        optimized_scenes = []
        
        for scene in scenes:
            # Adjust duration based on target
            if scene.duration > self.max_duration:
                # Split long scenes
                optimized_scenes.extend(self._split_long_scene(scene))
            elif scene.duration < self.min_duration:
                # Mark for potential merging
                scene.continuity_markers["merge_candidate"] = "true"
                optimized_scenes.append(scene)
            else:
                optimized_scenes.append(scene)
        
        # Merge short adjacent scenes if possible
        return self._merge_short_scenes(optimized_scenes)
    
    def _split_long_scene(self, scene: EnhancedVideoScene) -> List[EnhancedVideoScene]:
        """Split long scene into optimal chunks"""
        # Split by natural breaks
        sentences = re.split(r'[.!?]', scene.content)
        
        chunks = []
        current_chunk = ""
        current_duration = 0
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
            
            sentence_duration = len(sentence.split()) / 2.5
            
            if current_duration + sentence_duration <= self.target_duration:
                current_chunk += sentence + ". "
                current_duration += sentence_duration
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence + ". "
                current_duration = sentence_duration
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        # Create new scenes from chunks
        split_scenes = []
        for i, chunk in enumerate(chunks):
            new_scene = EnhancedVideoScene(
                id=scene.id + i * 0.1,  # Sub-scene numbering
                content=chunk,
                scene_type=scene.scene_type,
                duration=len(chunk.split()) / 2.5,
                cinematic_elements=scene.cinematic_elements,
                emotional_tone=scene.emotional_tone,
                pacing=scene.pacing
            )
            split_scenes.append(new_scene)
        
        return split_scenes
    
    def _merge_short_scenes(self, scenes: List[EnhancedVideoScene]) -> List[EnhancedVideoScene]:
        """Merge short adjacent scenes"""
        if not scenes:
            return scenes
        
        merged = []
        current_scene = scenes[0]
        
        for i in range(1, len(scenes)):
            next_scene = scenes[i]
            
            # Check if both scenes are merge candidates and compatible
            if (current_scene.continuity_markers.get("merge_candidate") == "true" and
                next_scene.continuity_markers.get("merge_candidate") == "true" and
                current_scene.scene_type == next_scene.scene_type and
                current_scene.duration + next_scene.duration <= self.max_duration):
                
                # Merge scenes
                merged_content = current_scene.content + " " + next_scene.content
                merged_duration = current_scene.duration + next_scene.duration
                
                current_scene.content = merged_content
                current_scene.duration = merged_duration
                current_scene.continuity_markers.pop("merge_candidate", None)
            else:
                merged.append(current_scene)
                current_scene = next_scene
        
        merged.append(current_scene)
        return merged
    
    def _analyze_continuity(self, scenes: List[EnhancedVideoScene]) -> List[EnhancedVideoScene]:
        """Analyze and mark continuity requirements"""
        
        for i, scene in enumerate(scenes):
            # Mark character continuity
            if i > 0:
                prev_scene = scenes[i-1]
                # Check for character overlap
                # This will be enhanced with character tracking
                scene.continuity_markers["previous_scene"] = str(prev_scene.id)
                
            # Mark setting continuity
            # This will be enhanced with setting analysis
            
            # Mark temporal continuity
            # This will be enhanced with temporal analysis
        
        return scenes

# Test enhanced scene analyzer
if __name__ == "__main__":
    analyzer = EnhancedSceneAnalyzer(target_duration=8.0, language="vietnamese")
    
    test_script = """
    FADE IN:
    
    EXT. NHÃ€ VIá»†T NAM - SÃNG Sá»šM
    
    Wide shot: Anh MINH (40 tuá»•i, Ä‘Ã n Ã´ng Viá»‡t Nam, dÃ¡ng gáº§y, da ngÄƒm Ä‘en, máº·t kháº¯c khá»•) 
    bÆ°á»›c ra khá»i nhÃ  tranh nhá». Ãnh náº¯ng sÃ¡ng sá»›m chiáº¿u rá»i lÃªn khuÃ´n máº·t nghiÃªm tÃºc cá»§a anh.
    
    CUT TO:
    
    EXT. CON ÄÆ¯á»œNG Äáº¤T - CONTINUOUS
    
    Medium shot: Anh Minh Ä‘i bá»™ nhanh chÃ³ng trÃªn con Ä‘Æ°á»ng Ä‘áº¥t giá»¯a cÃ¡nh Ä‘á»“ng lÃºa xanh.
    
    Close up: Anh dá»«ng láº¡i khi tháº¥y em LAN (25 tuá»•i, xinh Ä‘áº¹p, máº·c Ã¡o bÃ  ba tráº¯ng) Ä‘ang hÃ¡i rau.
    
    MINH
    (vui váº»)
    ChÃ o em Lan! SÃ¡ng nay em dáº­y sá»›m quÃ¡!
    
    LAN
    (cÆ°á»i tÆ°Æ¡i)
    ChÃ o anh Minh! Em pháº£i hÃ¡i rau sá»›m Ä‘á»ƒ mang ra chá»£ bÃ¡n.
    
    FADE OUT.
    """
    
    print("ğŸ¬ TESTING ENHANCED SCENE ANALYZER:")
    print("="*60)
    
    scenes = analyzer.analyze_script_enhanced(test_script)
    
    print(f"\nğŸ“Š ENHANCED ANALYSIS RESULTS:")
    print(f"   ğŸ¬ Total Scenes: {len(scenes)}")
    
    for i, scene in enumerate(scenes[:3]):  # Show first 3 scenes
        print(f"\nğŸ¯ Scene {scene.id}:")
        print(f"   Type: {scene.scene_type.value}")
        print(f"   Duration: {scene.duration:.1f}s")
        print(f"   Emotional Tone: {scene.emotional_tone}")
        print(f"   Pacing: {scene.pacing}")
        print(f"   Visual Complexity: {scene.visual_complexity:.2f}")
        print(f"   Cinematic Elements: {[e.value for e in scene.cinematic_elements]}")
        print(f"   Camera Angles: {scene.camera_angles}")
        print(f"   Recommended Shots: {scene.recommended_shots}")
        print(f"   Lighting: {scene.lighting_requirements}")
        print(f"   Sound: {scene.sound_design}")
        print(f"   Content: {scene.content[:100]}...")
    
    print(f"\nâœ… ENHANCED SCENE ANALYZER READY! ğŸ¬ğŸ¯")
